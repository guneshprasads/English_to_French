{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62f6b65-f436-453a-9946-0bfd52941d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d10b56-a393-4746-8beb-e667251b76db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Datasets/eng_-french.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f579c5d-eaf7-411c-a0fb-941fba651243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()                          # Lowercase\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)   # Remove punctuation\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()     # Remove extra spaces\n",
    "    tokens = word_tokenize(text)                 # Tokenize\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a99ef91-3d9b-4d3a-8745-33f3816f0f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['English words/sentences'] = df['English words/sentences'].apply(clean_text)\n",
    "df['French words/sentences'] = df['French words/sentences'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad5fae4f-4f6e-45ed-807a-2e330b086d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  English words/sentences French words/sentences\n",
      "0                      hi                  salut\n",
      "1                     run                  cours\n",
      "2                     run                 courez\n",
      "3                     who                    qui\n",
      "4                     wow                a alors\n"
     ]
    }
   ],
   "source": [
    "print(df[['English words/sentences', 'French words/sentences']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f62b6daa-51cc-4871-8f65-197e88e65d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_sentences = df['English words/sentences'].tolist()\n",
    "fr_sentences = df['French words/sentences'].tolist()\n",
    "\n",
    "eng_tokens = [sentence.split() for sentence in eng_sentences]\n",
    "fr_tokens = [sentence.split() for sentence in fr_sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8be8fdc7-0ece-4f91-a27c-491c2c36c655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(tokenized_sentences, min_freq=2):\n",
    "    counter = Counter()\n",
    "    for tokens in tokenized_sentences:\n",
    "        counter.update(tokens)\n",
    "\n",
    "    vocab = {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3}\n",
    "    index = 4\n",
    "    for word, freq in counter.items():\n",
    "        if freq >= min_freq:\n",
    "            vocab[word] = index\n",
    "            index += 1\n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab821eab-1b97-4d80-8909-7926cc9ee172",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_vocab = build_vocab(eng_tokens)\n",
    "fr_vocab = build_vocab(fr_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1fd7116d-faa9-4ddd-b6f1-79e7b8df4ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded English shape: torch.Size([175621, 45])\n",
      "Padded French shape: torch.Size([175621, 56])\n"
     ]
    }
   ],
   "source": [
    "def encode(sentence, vocab):\n",
    "    return [vocab.get(word, vocab['<UNK>']) for word in sentence] + [vocab['<EOS>']]\n",
    "\n",
    "encoded_eng = [torch.tensor(encode(tokens, eng_vocab)) for tokens in eng_tokens]\n",
    "encoded_fr = [torch.tensor([fr_vocab['<SOS>']] + encode(tokens, fr_vocab)) for tokens in fr_tokens]\n",
    "\n",
    "padded_eng = pad_sequence(encoded_eng, batch_first=True, padding_value=eng_vocab['<PAD>'])\n",
    "padded_fr = pad_sequence(encoded_fr, batch_first=True, padding_value=fr_vocab['<PAD>'])\n",
    "\n",
    "print(\"Padded English shape:\", padded_eng.shape)\n",
    "print(\"Padded French shape:\", padded_fr.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0fe193f-7a17-40f8-ba8e-e11a308fbe0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, source_tensor, target_tensor):\n",
    "        self.source = source_tensor\n",
    "        self.target = target_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.source[idx], self.target[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "daa1d49b-f121-45ee-b03b-e1ee6733fb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TranslationDataset(padded_eng, padded_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f753f1d-8cfe-4e98-b3d2-e08520301763",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fcbd1556-786c-4064-85c3-f408c4261a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoder RNN\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim, num_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.gru = nn.GRU(emb_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        # src: [batch_size, seq_len]\n",
    "        embedded = self.embedding(src)  # [batch_size, seq_len, emb_dim]\n",
    "        outputs, hidden = self.gru(embedded)  # outputs: [batch_size, seq_len, hidden_dim], hidden: [num_layers, batch_size, hidden_dim]\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2f94ba45-36f0-4667-9419-3b3bb572ab9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decoder RNN\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, output_dim, embed_dim, hidden_dim, num_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, embed_dim)\n",
    "        self.rnn = nn.GRU(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # input shape: (batch_size) â€” single token input\n",
    "        input = input.unsqueeze(1)  # shape -> (batch_size, 1)\n",
    "        embedded = self.embedding(input)  # (batch_size, 1, embed_dim)\n",
    "        output, hidden = self.rnn(embedded, hidden)  # output: (batch_size, 1, hidden_dim)\n",
    "        prediction = self.fc_out(output.squeeze(1))  # (batch_size, output_dim)\n",
    "        return prediction, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "18a0f1d6-6130-487a-b74c-81cefd295670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.size(0)\n",
    "        trg_len = trg.size(1)\n",
    "        trg_vocab_size = self.decoder.fc_out.out_features\n",
    "\n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
    "\n",
    "        # Step 1: Encode the source sentence\n",
    "        hidden = self.encoder(src)\n",
    "\n",
    "        # Step 2: First input to the decoder is <SOS>\n",
    "        input = trg[:, 0]\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden = self.decoder(input, hidden)  # One step\n",
    "            outputs[:, t] = output\n",
    "\n",
    "            # Decide: use teacher forcing or predicted word?\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)  # Get index of highest probability\n",
    "\n",
    "            input = trg[:, t] if teacher_force else top1\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e0d2b601-26a4-4a17-872a-69bc95782d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set hyperparameters\n",
    "INPUT_DIM = len(eng_vocab)\n",
    "OUTPUT_DIM = len(fr_vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate encoder, decoder, and Seq2Seq model\n",
    "encoder = EncoderRNN(INPUT_DIM, ENC_EMB_DIM, HID_DIM)\n",
    "decoder = DecoderRNN(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM)\n",
    "seq2seq = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = optim.Adam(seq2seq.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=fr_vocab['<PAD>'])\n",
    "\n",
    "# Define training loop\n",
    "def train(model, dataloader, optimizer, criterion, device, num_epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        for src, tgt in dataloader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(src, tgt)\n",
    "            \n",
    "            # Reshape output and target to compute loss\n",
    "            output = output[:, 1:].reshape(-1, output.shape[-1])\n",
    "            tgt = tgt[:, 1:].reshape(-1)\n",
    "            \n",
    "            loss = criterion(output, tgt)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {epoch_loss/len(dataloader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104af1fd-c8f1-4dde-a22e-96951c1f0105",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(seq2seq, dataloader, optimizer, criterion, device, num_epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd75719-f72a-4b84-ae94-7160e0380861",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(model, sentence, eng_vocab, fr_vocab, device, max_len=20):\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize and encode input sentence\n",
    "    tokens = sentence.lower().split()\n",
    "    encoded = [eng_vocab.get(token, eng_vocab['<UNK>']) for token in tokens]\n",
    "    input_tensor = torch.tensor(encoded).unsqueeze(0).to(device)  # shape: (1, seq_len)\n",
    "\n",
    "    # Pass through encoder\n",
    "    with torch.no_grad():\n",
    "        hidden = model.encoder(input_tensor)\n",
    "\n",
    "    # Decoder start with <SOS>\n",
    "    outputs = [fr_vocab['<SOS>']]\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        prev_word = torch.tensor([outputs[-1]]).unsqueeze(0).to(device)  # shape: (1, 1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output, hidden = model.decoder(prev_word, hidden)\n",
    "            pred_token = output.argmax(2).item()  # Get the most likely word\n",
    "\n",
    "        outputs.append(pred_token)\n",
    "\n",
    "        if pred_token == fr_vocab['<EOS>']:\n",
    "            break\n",
    "\n",
    "    # Invert fr_vocab to convert ids back to words\n",
    "    idx2word = {idx: word for word, idx in fr_vocab.items()}\n",
    "    translated = [idx2word.get(idx, '<UNK>') for idx in outputs[1:]]  # Skip <SOS>\n",
    "\n",
    "    return ' '.join(translated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea6e8f8-3058-4a3d-ac0d-279811a4db3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"i like cats\"\n",
    "translation = translate_sentence(seq2seq, example, eng_vocab, fr_vocab, device)\n",
    "print(f\"English: {example}\")\n",
    "print(f\"French : {translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9246a973-667b-4d98-9297-72846b8c469e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f420acf-a535-453a-a942-cb34d61bb2cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
